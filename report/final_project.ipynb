{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f20dd4d-0635-45d3-b11f-82c9f67e6562",
   "metadata": {},
   "source": [
    "# DSCI100 Final Group Project\n",
    "\n",
    "## Data Description:\n",
    "\n",
    "The datasets given are these two files: players.csv and sessions.csv.\n",
    "\n",
    "players.csv contains 197 observations of:\n",
    "experience: Player’s chosen experience level, such as Pro, Veteran, Amateur, etc.\n",
    "subscribe: Indicates if the player has subscribed to the email notification (True/False).\n",
    "played_hours: Total hours played by the player.\n",
    "gender: Gender of the player.\n",
    "age: Age of the player.\n",
    "\n",
    "sessions.csv contains 1536 (and counting) observations of:\n",
    "hashedEmail: Anonymized identifier linking sessions to players.\n",
    "start_time and end_time: Start and end times of each play session.\n",
    "original_start_time and original_end_time: Timestamps for each session's start and end.\n",
    "\n",
    "We plan to combine the information given from both files to calculate player metrics like session frequency, average session length, total playtime and time of day or day of week patterns. There is missing data in players.csv (individualId, organizationName) and there are outliers and inconsistencies in session.csv which will be addressed through data preprocessing, like standardizing the time formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6d592-1c02-45e1-af67-1f55c91e64c7",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Question: \"What time windows are most likely to have large number of simultaneous players>\"\n",
    "\n",
    "The goal is to predict the time window when the demand for concurrent player licenses is based on past patterns. We can analyze the hour of the day as well as the day of the week that most players get on, to help forecast player demand in each time window. This can help the research team manage the licenses and server capacity effectively during peak hours.\n",
    "\n",
    "The predictors are:\n",
    "\n",
    "Hour of the day, Day of the week, Weekend or Weekday\n",
    "\n",
    "\n",
    "We will standardize all the time variables so that it can be easily plotted and analyzed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c9912-58b0-4024-b359-50a30e21b356",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Visualization\n",
    "\n",
    "To gain insight into player demand patterns, we’ll:\n",
    "\n",
    "Aggregate data to determine the number of simultaneous players for each hourly window.\n",
    "Visualize how player counts vary by hour of the day and day of the week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07113164-2971-4692-90e2-cdfb75aee6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.5\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.1     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: 'data/players.csv' does not exist in current working directory ('/home/jovyan/work/DSCI-Final-Group-13/report').\n",
     "output_type": "error",
     "traceback": [
      "Error: 'data/players.csv' does not exist in current working directory ('/home/jovyan/work/DSCI-Final-Group-13/report').\nTraceback:\n",
      "1. read_csv(\"data/players.csv\")",
      "2. vroom::vroom(file, delim = \",\", col_names = col_names, col_types = col_types, \n .     col_select = {\n .         {\n .             col_select\n .         }\n .     }, id = id, .name_repair = name_repair, skip = skip, n_max = n_max, \n .     na = na, quote = quote, comment = comment, skip_empty_rows = skip_empty_rows, \n .     trim_ws = trim_ws, escape_double = TRUE, escape_backslash = FALSE, \n .     locale = locale, guess_max = guess_max, show_col_types = show_col_types, \n .     progress = progress, altrep = lazy, num_threads = num_threads)",
      "3. vroom_(file, delim = delim %||% col_types$delim, col_names = col_names, \n .     col_types = col_types, id = id, skip = skip, col_select = col_select, \n .     name_repair = .name_repair, na = na, quote = quote, trim_ws = trim_ws, \n .     escape_double = escape_double, escape_backslash = escape_backslash, \n .     comment = comment, skip_empty_rows = skip_empty_rows, locale = locale, \n .     guess_max = guess_max, n_max = n_max, altrep = vroom_altrep(altrep), \n .     num_threads = num_threads, progress = progress)",
      "4. (function (path, write = FALSE) \n . {\n .     if (is.raw(path)) {\n .         return(rawConnection(path, \"rb\"))\n .     }\n .     if (!is.character(path)) {\n .         return(path)\n .     }\n .     if (is_url(path)) {\n .         if (requireNamespace(\"curl\", quietly = TRUE)) {\n .             con <- curl::curl(path)\n .         }\n .         else {\n .             inform(\"`curl` package not installed, falling back to using `url()`\")\n .             con <- url(path)\n .         }\n .         ext <- tolower(tools::file_ext(path))\n .         return(switch(ext, zip = , bz2 = , xz = {\n .             close(con)\n .             stop(\"Reading from remote `\", ext, \"` compressed files is not supported,\\n\", \n .                 \"  download the files locally first.\", call. = FALSE)\n .         }, gz = gzcon(con), con))\n .     }\n .     path <- enc2utf8(path)\n .     p <- split_path_ext(basename_utf8(path))\n .     if (write) {\n .         path <- normalizePath_utf8(path, mustWork = FALSE)\n .     }\n .     else {\n .         path <- check_path(path)\n .     }\n .     if (is_installed(\"archive\")) {\n .         formats <- archive_formats(p$extension)\n .         extension <- p$extension\n .         while (is.null(formats) && nzchar(extension)) {\n .             extension <- split_path_ext(extension)$extension\n .             formats <- archive_formats(extension)\n .         }\n .         if (!is.null(formats)) {\n .             p$extension <- extension\n .             if (write) {\n .                 if (is.null(formats[[1]])) {\n .                   return(archive::file_write(path, filter = formats[[2]]))\n .                 }\n .                 return(archive::archive_write(path, p$path, format = formats[[1]], \n .                   filter = formats[[2]]))\n .             }\n .             if (is.null(formats[[1]])) {\n .                 return(archive::file_read(path, filter = formats[[2]]))\n .             }\n .             return(archive::archive_read(path, format = formats[[1]], \n .                 filter = formats[[2]]))\n .         }\n .     }\n .     if (!write) {\n .         compression <- detect_compression(path)\n .     }\n .     else {\n .         compression <- NA\n .     }\n .     if (is.na(compression)) {\n .         compression <- tools::file_ext(path)\n .     }\n .     if (write && compression == \"zip\") {\n .         stop(\"Can only read from, not write to, .zip\", call. = FALSE)\n .     }\n .     switch(compression, gz = gzfile(path, \"\"), bz2 = bzfile(path, \n .         \"\"), xz = xzfile(path, \"\"), zip = zipfile(path, \"\"), \n .         if (!has_trailing_newline(path)) {\n .             file(path)\n .         } else {\n .             path\n .         })\n . })(\"data/players.csv\")",
      "5. check_path(path)",
      "6. stop(\"'\", path, \"' does not exist\", if (!is_absolute_path(path)) {\n .     paste0(\" in current working directory ('\", getwd(), \"')\")\n . }, \".\", call. = FALSE)"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "players_url <- \"https://drive.google.com/uc?export=download&id=1Mw9vW0hjTJwRWx0bDXiSpYsO3gKogaPz\"\n",
    "sessions_url <- \"https://drive.google.com/uc?export=download&id=14O91N5OlVkvdGxXNJUj5jIsV5RexhzbB\"\n",
    "download.file(players_url, destfile = \"players.csv\", mode = \"wb\")\n",
    "download.file(sessions_url, destfile = \"sessions.csv\", mode = \"wb\")\n",
    "\n",
    "players <- read_csv(\"data/players.csv\")\n",
    "sessions <- read_csv(\"data/sessions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4140dae9-2bc5-4023-a73a-966c0ae4d049",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'merged_data' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'merged_data' not found\nTraceback:\n",
      "1. merged_data %>% mutate(start_datetime = as.POSIXct(start_time, \n .     format = \"%d/%m/%Y %H:%M\"), end_datetime = as.POSIXct(end_time, \n .     format = \"%d/%m/%Y %H:%M\"))",
      "2. mutate(., start_datetime = as.POSIXct(start_time, format = \"%d/%m/%Y %H:%M\"), \n .     end_datetime = as.POSIXct(end_time, format = \"%d/%m/%Y %H:%M\"))"
     ]
    }
   ],
   "source": [
    "#format data to date month year hour minute\n",
    "merged_data <- merged_data %>%\n",
    "  mutate(\n",
    "    start_datetime = as.POSIXct(start_time, format = \"%d/%m/%Y %H:%M\"),\n",
    "    end_datetime = as.POSIXct(end_time, format = \"%d/%m/%Y %H:%M\")\n",
    "  )\n",
    "#make new df to show time windows\n",
    "#standardaize start times to within the same hour\n",
    "#count simul players during said time\n",
    "time_windows <- merged_data %>%\n",
    "  mutate(hour = format(start_datetime, \"%Y-%m-%d %H:00:00\")) %>%\n",
    "  group_by(hour) %>%\n",
    "  summarise(simultaneous_players = n())\n",
    "#convert back to time format for plotting\n",
    "time_windows <- time_windows %>%\n",
    "  mutate(hour = as.POSIXct(hour, format = \"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ed0b-7223-4ee2-812f-cdfd5118504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(time_windows, aes(x = hour, y = simultaneous_players)) +\n",
    "  geom_line() +\n",
    "  labs(\n",
    "    title = \"Simultaneous Player Counts Over Time\",\n",
    "    x = \"Time (Hourly)\",\n",
    "    y = \"Number of Simultaneous Players\"\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eee00d-0aa9-4658-a303-bc4dfcb0d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add hour of day \n",
    "time_windows <- time_windows %>%\n",
    "  mutate(hour_of_day = as.integer(format(hour, \"%H\")))\n",
    "\n",
    "#calculte simul players during time of day (24 hr format)\n",
    "avg_demand_by_hour <- time_windows %>%\n",
    "  group_by(hour_of_day) %>%\n",
    "  summarise(avg_players = mean(simultaneous_players, na.rm = TRUE))\n",
    "\n",
    "# avg simul player vs hour of day\n",
    "ggplot(avg_demand_by_hour, aes(x = hour_of_day, y = avg_players)) +\n",
    "  geom_line() +\n",
    "  labs(\n",
    "    title = \"Average Player Demand by Hour of Day\",\n",
    "    x = \"Hour of Day\",\n",
    "    y = \"Average Simultaneous Players\"\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7e5bc-fec8-4887-9542-2387b6fc50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add day of week\n",
    "time_windows <- time_windows %>%\n",
    "  mutate(day_of_week = weekdays(hour))\n",
    "# count avg player simultaneous count\n",
    "avg_demand_by_day <- time_windows %>%\n",
    "  group_by(day_of_week) %>%\n",
    "  summarise(avg_players = mean(simultaneous_players, na.rm = TRUE))\n",
    "\n",
    "#plot average simul players vs day of week\n",
    "ggplot(avg_demand_by_day, aes(x = day_of_week, y = avg_players)) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  labs(\n",
    "    title = \"Average Player Demand by Day of Week\",\n",
    "    x = \"Day of Week\",\n",
    "    y = \"Average Simultaneous Players\"\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f6bc0-d2d9-4d80-974f-e6158b3a46c2",
   "metadata": {},
   "source": [
    "## (4) Methods and Plan\n",
    "\n",
    "We are using linear regression as an appropriate method because of the nature of the problem. This method can capture trends and patterns based on time-related variables like hour of the day and day of the week. It is also easily interpreted and allows see the demand for licenses. Using this model can allow us to predict the expected player count in each time window. We are assuming a linear relationship between our predictors and response variable so that this model can work. We assume that the player count i0n one hour is independent of player counts in other hours. The limitation of this model and plan is the possible sudden spikes in player counts that can occur at any time(As seen in Simultaneous Player Counts over Time). The data may also be showing the same player over different time windows. This means that if there is an average of 1 player count but its all different people then different licenses are still required, but the model doesn't know that. We may test the model accuracy using RMSE on the training set, and RMSPE on the test set. We will split the data to 70% training, and use the later/ more recent data periods to test. We will use K-Fold Cross Validation on the training set. We can use this plan to forecast peak demand times and allow to accomodate all parallel players. This approach offers a clear and understandable solution, while also being flexible on tweaking it based on how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ef82c-8478-4a6a-8bc6-9e602d3cc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de66dc-94e6-4297-8dbf-2e22e2a5377c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
